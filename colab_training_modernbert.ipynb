{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1e0ff9",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Text-Machine/data-processing-code/blob/main/colab_training_modernbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688592c",
   "metadata": {},
   "source": [
    "# ModernBERT Pretraining on Historical Texts (Google Colab)\n",
    "\n",
    "This notebook allows you to pretrain ModernBERT on historical text data (EEBO, ECCO, EVAN) using Google Colab's free GPU.\n",
    "\n",
    "## Setup Steps:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)\n",
    "2. **Upload Data**: Upload your CSV files to Google Drive or upload directly\n",
    "3. **Run All Cells**: Runtime ‚Üí Run all\n",
    "\n",
    "## What this does:\n",
    "- Installs required packages\n",
    "- Mounts Google Drive (optional)\n",
    "- Loads CSV data with columns: `author`, `place`, `date`, `page_text`\n",
    "- Chunks text into 250-token segments with `<date> [TIME]` prefix\n",
    "- Trains ModernBERT with masked language modeling\n",
    "- Saves trained model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71749c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install -q transformers datasets pandas accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda77b29",
   "metadata": {},
   "source": [
    "## Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5918d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 11wfdV7j1TBv_i9XOiT8G8V4NxnJTxezz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cc94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2da70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create data directory\n",
    "DATA_DIR = 'data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "OUTPUT_DIR = 'output/modernbert_pretrained'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "\n",
    "def load_csv_as_dataset(csv_paths):\n",
    "    \"\"\"Load CSV files and convert to Hugging Face Dataset.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        logger.info(f\"Loading {Path(csv_path).name}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"  Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
    "        all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    logger.info(f\"Total rows: {len(combined_df)}\")\n",
    "    \n",
    "    dataset = Dataset.from_pandas(combined_df)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_chunk_function(examples, tokenizer, max_chunk_length=250):\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    time_id = tokenizer.convert_tokens_to_ids(\"[TIME]\")\n",
    "    max_len = tokenizer.model_max_length\n",
    "\n",
    "    for date, text in zip(examples[\"date\"], examples[\"page_text\"]):\n",
    "\n",
    "        if not text or pd.isna(text) or pd.isna(date):\n",
    "            continue\n",
    "\n",
    "        date_ids = tokenizer.encode(str(date).strip(), add_special_tokens=False)\n",
    "        text_ids = tokenizer.encode(str(text).strip(), add_special_tokens=False)\n",
    "\n",
    "        reserved = 1 + len(date_ids) + 1 + 1\n",
    "        max_text_len = max_len - reserved\n",
    "        if max_text_len <= 0:\n",
    "            continue\n",
    "\n",
    "        chunk_size = min(max_chunk_length, max_text_len)\n",
    "\n",
    "        for start in range(0, len(text_ids), chunk_size):\n",
    "            chunk = text_ids[start:start + chunk_size]\n",
    "\n",
    "            ids = [cls_id] + date_ids + [time_id] + chunk + [sep_id]\n",
    "            ids = ids[:max_len]\n",
    "\n",
    "            input_ids_list.append(ids)\n",
    "            attention_masks_list.append([1] * len(ids))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_masks_list,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c24e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHUNK_LENGTH = 250\n",
    "BATCH_SIZE = 16  # Reduce if you run out of memory\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SAMPLES = None  # Set to e.g., 10000 for quick testing\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"  # ModernBERT base model\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Chunk length: {CHUNK_LENGTH} tokens\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES or 'All'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = '.'\n",
    "csv_files = list(Path(DATA_DIR).glob('*.csv'))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}. Please upload data first.\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "dataset = load_csv_as_dataset(csv_files)\n",
    "print(f\"\\nDataset loaded: {len(dataset)} rows\")\n",
    "\n",
    "# Limit samples if specified\n",
    "if MAX_SAMPLES and MAX_SAMPLES < len(dataset):\n",
    "    dataset = dataset.select(range(MAX_SAMPLES))\n",
    "    print(f\"Limited to {MAX_SAMPLES} samples for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(f\"Loading ModernBERT tokenizer and model from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add [TIME] special token\n",
    "if \"[TIME]\" not in tokenizer.vocab:\n",
    "    tokenizer.add_tokens([\"[TIME]\"])\n",
    "    print(\"Added [TIME] token to vocabulary\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data (tokenize and chunk)\n",
    "print(\"Tokenizing and chunking text (this may take several minutes)...\")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenize_and_chunk_function(\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        max_chunk_length=CHUNK_LENGTH\n",
    "    ),\n",
    "    batched=True,\n",
    "    batch_size=50,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1,\n",
    "    desc=\"Tokenizing and chunking\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)} samples\")\n",
    "\n",
    "# Show sample\n",
    "if len(tokenized_dataset) > 0:\n",
    "    sample = tokenized_dataset[0]\n",
    "    print(f\"\\nSample input (first 100 tokens):\")\n",
    "    print(tokenizer.decode(sample['input_ids'][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3247bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "OUTPUT_DIR = 'modernbert_pretrained'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer configured\")\n",
    "print(f\"\\nStarting training with {EPOCHS} epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0138b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94943b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.push_to_hub()\n",
    "\n",
    "print(\"\\n‚úì Model saved successfully!\")\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"  from transformers import AutoModelForMaskedLM, AutoTokenizer\")\n",
    "print(f\"  model = AutoModelForMaskedLM.from_pretrained('{OUTPUT_DIR}')\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{OUTPUT_DIR}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500da633",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "Let's test the model with masked language modeling predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MLM predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test sentences with historical context\n",
    "test_sentences = [\n",
    "    \"1650 [TIME]  The [MASK] returned to parliament.\",\n",
    "    \"1800 [TIME]  The [MASK] returned to parliament.\",\n",
    "]\n",
    "\n",
    "print(\"Testing masked language model predictions:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    print(f\"Input: {sentence}\")\n",
    "    predictions = fill_mask(sentence, top_k=3)\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {pred['token_str']:>12} (score: {pred['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b1781",
   "metadata": {},
   "source": [
    "## Download Model (Optional)\n",
    "\n",
    "If you want to download the trained model to your local machine, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94034e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download model\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create zip file\n",
    "zip_path = '/content/modernbert_pretrained'\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model zipped. Size: {Path(f'{zip_path}.zip').stat().st_size / 1e6:.1f} MB\")\n",
    "print(\"Downloading...\")\n",
    "\n",
    "# Download\n",
    "files.download(f'{zip_path}.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d85c8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Text-Machine/data-processing-code/blob/main/colab_training_modernbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6e167",
   "metadata": {},
   "source": [
    "# ModernBERT Pretraining on Historical Texts (Google Colab)\n",
    "\n",
    "This notebook allows you to pretrain ModernBERT on historical text data (EEBO, ECCO, EVAN) using Google Colab's free GPU.\n",
    "\n",
    "## Setup Steps:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)\n",
    "2. **Upload Data**: Upload your CSV files to Google Drive or upload directly\n",
    "3. **Run All Cells**: Runtime ‚Üí Run all\n",
    "\n",
    "## What this does:\n",
    "- Installs required packages\n",
    "- Mounts Google Drive (optional)\n",
    "- Loads CSV data with columns: `author`, `place`, `date`, `page_text`\n",
    "- Chunks text into 250-token segments with `<date> [TIME]` prefix\n",
    "- Trains ModernBERT with masked language modeling\n",
    "- Saves trained model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87704d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -q transformers datasets pandas accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811b211",
   "metadata": {},
   "source": [
    "## Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 11wfdV7j1TBv_i9XOiT8G8V4NxnJTxezz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536da3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create data directory\n",
    "DATA_DIR = 'data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "OUTPUT_DIR = 'output/modernbert_pretrained'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "\n",
    "def load_csv_as_dataset(csv_paths):\n",
    "    \"\"\"Load CSV files and convert to Hugging Face Dataset.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        logger.info(f\"Loading {Path(csv_path).name}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"  Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
    "        all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    logger.info(f\"Total rows: {len(combined_df)}\")\n",
    "    \n",
    "    dataset = Dataset.from_pandas(combined_df)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_chunk_function(examples, tokenizer, max_chunk_length=250):\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    time_id = tokenizer.convert_tokens_to_ids(\"[TIME]\")\n",
    "    max_len = tokenizer.model_max_length\n",
    "\n",
    "    for date, text in zip(examples[\"date\"], examples[\"page_text\"]):\n",
    "\n",
    "        if not text or pd.isna(text) or pd.isna(date):\n",
    "            continue\n",
    "\n",
    "        date_ids = tokenizer.encode(str(date).strip(), add_special_tokens=False)\n",
    "        text_ids = tokenizer.encode(str(text).strip(), add_special_tokens=False)\n",
    "\n",
    "        reserved = 1 + len(date_ids) + 1 + 1\n",
    "        max_text_len = max_len - reserved\n",
    "        if max_text_len <= 0:\n",
    "            continue\n",
    "\n",
    "        chunk_size = min(max_chunk_length, max_text_len)\n",
    "\n",
    "        for start in range(0, len(text_ids), chunk_size):\n",
    "            chunk = text_ids[start:start + chunk_size]\n",
    "\n",
    "            ids = [cls_id] + date_ids + [time_id] + chunk + [sep_id]\n",
    "            ids = ids[:max_len]\n",
    "\n",
    "            input_ids_list.append(ids)\n",
    "            attention_masks_list.append([1] * len(ids))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_masks_list,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHUNK_LENGTH = 250\n",
    "BATCH_SIZE = 16  # Reduce if you run out of memory\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SAMPLES = None  # Set to e.g., 10000 for quick testing\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"  # ModernBERT base model\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Chunk length: {CHUNK_LENGTH} tokens\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES or 'All'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc05d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = '.'\n",
    "csv_files = list(Path(DATA_DIR).glob('*.csv'))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}. Please upload data first.\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "dataset = load_csv_as_dataset(csv_files)\n",
    "print(f\"\\nDataset loaded: {len(dataset)} rows\")\n",
    "\n",
    "# Limit samples if specified\n",
    "if MAX_SAMPLES and MAX_SAMPLES < len(dataset):\n",
    "    dataset = dataset.select(range(MAX_SAMPLES))\n",
    "    print(f\"Limited to {MAX_SAMPLES} samples for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f87a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(f\"Loading ModernBERT tokenizer and model from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add [TIME] special token\n",
    "if \"[TIME]\" not in tokenizer.vocab:\n",
    "    tokenizer.add_tokens([\"[TIME]\"])\n",
    "    print(\"Added [TIME] token to vocabulary\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data (tokenize and chunk)\n",
    "print(\"Tokenizing and chunking text (this may take several minutes)...\")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenize_and_chunk_function(\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        max_chunk_length=CHUNK_LENGTH\n",
    "    ),\n",
    "    batched=True,\n",
    "    batch_size=50,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1,\n",
    "    desc=\"Tokenizing and chunking\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)} samples\")\n",
    "\n",
    "# Show sample\n",
    "if len(tokenized_dataset) > 0:\n",
    "    sample = tokenized_dataset[0]\n",
    "    print(f\"\\nSample input (first 100 tokens):\")\n",
    "    print(tokenizer.decode(sample['input_ids'][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72aade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "OUTPUT_DIR = 'modernbert_pretrained'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer configured\")\n",
    "print(f\"\\nStarting training with {EPOCHS} epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63bf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.push_to_hub()\n",
    "\n",
    "print(\"\\n‚úì Model saved successfully!\")\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"  from transformers import AutoModelForMaskedLM, AutoTokenizer\")\n",
    "print(f\"  model = AutoModelForMaskedLM.from_pretrained('{OUTPUT_DIR}')\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{OUTPUT_DIR}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ce9f4",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "Let's test the model with masked language modeling predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MLM predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test sentences with historical context\n",
    "test_sentences = [\n",
    "    \"1650 [TIME] The [MASK] returned to parliament.\",\n",
    "    \"1800 [TIME] The [MASK] returned to parliament.\",\n",
    "]\n",
    "\n",
    "print(\"Testing masked language model predictions:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    print(f\"Input: {sentence}\")\n",
    "    predictions = fill_mask(sentence, top_k=3)\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {pred['token_str']:>12} (score: {pred['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3ce8d",
   "metadata": {},
   "source": [
    "## Download Model (Optional)\n",
    "\n",
    "If you want to download the trained model to your local machine, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e09f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download model\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create zip file\n",
    "zip_path = '/content/modernbert_pretrained'\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model zipped. Size: {Path(f'{zip_path}.zip').stat().st_size / 1e6:.1f} MB\")\n",
    "print(\"Downloading...\")\n",
    "\n",
    "# Download\n",
    "files.download(f'{zip_path}.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e7e30",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Text-Machine/data-processing-code/blob/main/colab_training_modernbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b68c74",
   "metadata": {},
   "source": [
    "# ModernBERT Pretraining on Historical Texts (Google Colab)\n",
    "\n",
    "This notebook allows you to pretrain ModernBERT on historical text data (EEBO, ECCO, EVAN) using Google Colab's free GPU.\n",
    "\n",
    "## Setup Steps:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 recommended)\n",
    "2. **Upload Data**: Upload your CSV files to Google Drive or upload directly\n",
    "3. **Run All Cells**: Runtime ‚Üí Run all\n",
    "\n",
    "## What this does:\n",
    "- Installs required packages\n",
    "- Mounts Google Drive (optional)\n",
    "- Loads CSV data with columns: `author`, `place`, `date`, `page_text`\n",
    "- Chunks text into 250-token segments with `<date> [TIME]` prefix\n",
    "- Trains ModernBERT with masked language modeling\n",
    "- Saves trained model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install -q transformers datasets pandas accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f487e1",
   "metadata": {},
   "source": [
    "## Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 11wfdV7j1TBv_i9XOiT8G8V4NxnJTxezz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabf8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create data directory\n",
    "# Update paths\n",
    "DATA_DIR = 'data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "OUTPUT_DIR = 'output/modernbert_pretrained'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "\n",
    "def load_csv_as_dataset(csv_paths):\n",
    "    \"\"\"Load CSV files and convert to Hugging Face Dataset.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        logger.info(f\"Loading {Path(csv_path).name}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"  Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
    "        all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    logger.info(f\"Total rows: {len(combined_df)}\")\n",
    "    \n",
    "    dataset = Dataset.from_pandas(combined_df)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_chunk_function(examples, tokenizer, max_chunk_length=250):\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    sep_id = tokenizer.sep_token_id\n",
    "    time_id = tokenizer.convert_tokens_to_ids(\"[TIME]\")\n",
    "    max_len = tokenizer.model_max_length\n",
    "\n",
    "    for date, text in zip(examples[\"date\"], examples[\"page_text\"]):\n",
    "\n",
    "        if not text or pd.isna(text) or pd.isna(date):\n",
    "            continue\n",
    "\n",
    "        date_ids = tokenizer.encode(str(date).strip(), add_special_tokens=False)\n",
    "        text_ids = tokenizer.encode(str(text).strip(), add_special_tokens=False)\n",
    "\n",
    "        reserved = 1 + len(date_ids) + 1 + 1\n",
    "        max_text_len = max_len - reserved\n",
    "        if max_text_len <= 0:\n",
    "            continue\n",
    "\n",
    "        chunk_size = min(max_chunk_length, max_text_len)\n",
    "\n",
    "        for start in range(0, len(text_ids), chunk_size):\n",
    "            chunk = text_ids[start:start + chunk_size]\n",
    "\n",
    "            ids = [cls_id] + date_ids + [time_id] + chunk + [sep_id]\n",
    "            ids = ids[:max_len]\n",
    "\n",
    "            input_ids_list.append(ids)\n",
    "            attention_masks_list.append([1] * len(ids))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_masks_list,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9742f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHUNK_LENGTH = 250\n",
    "BATCH_SIZE = 16  # Reduce if you run out of memory\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SAMPLES = None  # Set to e.g., 10000 for quick testing\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"  # ModernBERT base model\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Chunk length: {CHUNK_LENGTH} tokens\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES or 'All'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = '.'\n",
    "csv_files = list(Path(DATA_DIR).glob('*.csv'))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}. Please upload data first.\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "dataset = load_csv_as_dataset(csv_files)\n",
    "print(f\"\\nDataset loaded: {len(dataset)} rows\")\n",
    "\n",
    "# Limit samples if specified\n",
    "if MAX_SAMPLES and MAX_SAMPLES < len(dataset):\n",
    "    dataset = dataset.select(range(MAX_SAMPLES))\n",
    "    print(f\"Limited to {MAX_SAMPLES} samples for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c22729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(f\"Loading ModernBERT tokenizer and model from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add [TIME] special token\n",
    "if \"[TIME]\" not in tokenizer.vocab:\n",
    "    tokenizer.add_tokens([\"[TIME]\"])\n",
    "    print(\"Added [TIME] token to vocabulary\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data (tokenize and chunk)\n",
    "print(\"Tokenizing and chunking text (this may take several minutes)...\")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenize_and_chunk_function(\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        max_chunk_length=CHUNK_LENGTH\n",
    "    ),\n",
    "    batched=True,\n",
    "    batch_size=50,\n",
    "    remove_columns=dataset.column_names,   # üí• remove EVERYTHING old\n",
    "    num_proc=1,\n",
    "    desc=\"Tokenizing and chunking\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)} samples\")\n",
    "\n",
    "# Show sample\n",
    "if len(tokenized_dataset) > 0:\n",
    "    sample = tokenized_dataset[0]\n",
    "    print(f\"\\nSample input (first 100 tokens):\")\n",
    "    print(tokenizer.decode(sample['input_ids'][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757677b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecfbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "OUTPUT_DIR = 'modernbert_pretrained'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    #overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer configured\")\n",
    "print(f\"\\nStarting training with {EPOCHS} epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "# model.push_to_hub()\n",
    "# tokenizer.push_to_hub()\n",
    "trainer.push_to_hub()\n",
    "\n",
    "print(\"\\n‚úì Model saved successfully!\")\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"  from transformers import AutoModelForMaskedLM, AutoTokenizer\")\n",
    "print(f\"  model = AutoModelForMaskedLM.from_pretrained('{OUTPUT_DIR}')\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{OUTPUT_DIR}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce050fb9",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "Let's test the model with masked language modeling predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8850939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MLM predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test sentences with historical context\n",
    "test_sentences = [\n",
    "    \"1650 [TIME]  The [MASK] returned to parliament.\",\n",
    "    \"1800 [TIME]  The [MASK] returned to parliament.\",\n",
    "]\n",
    "\n",
    "print(\"Testing masked language model predictions:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    print(f\"Input: {sentence}\")\n",
    "    predictions = fill_mask(sentence, top_k=3)\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {pred['token_str']:>12} (score: {pred['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53328997",
   "metadata": {},
   "source": [
    "## Download Model (Optional)\n",
    "\n",
    "If you want to download the trained model to your local machine, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29602a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download model\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create zip file\n",
    "zip_path = '/content/modernbert_pretrained'\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model zipped. Size: {Path(f'{zip_path}.zip').stat().st_size / 1e6:.1f} MB\")\n",
    "print(\"Downloading...\")\n",
    "\n",
    "# Download\n",
    "files.download(f'{zip_path}.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
