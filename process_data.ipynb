{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc77e477",
   "metadata": {},
   "source": [
    "# BERT Pretraining on Historical Text Data\n",
    "\n",
    "Step-by-step walkthrough of pretraining BERT on EEBO/ECCO/EVAN historical text data.\n",
    "\n",
    "**Key features:**\n",
    "- Load data from CSV with pandas\n",
    "- Efficient preprocessing with Hugging Face Datasets (dynamic padding)\n",
    "- Chunk text into 250-token sequences with date prefix + [TIME] token\n",
    "- Apply masking to both date and text\n",
    "- Train using Trainer API with validation monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef9b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path exists: True\n",
      "Found 5012 XML files\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224db2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parser on: N00001.p4.xml\n",
      "\n",
      "Extracted 49 pages\n",
      "\n",
      "First page sample:\n",
      "    author                place             date                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           page_text\n",
      "0  Unknown  [Cambridge, Mass. :  Imprinted 1640.  THE VVHOLE BOOKE OF PSALMES Faithfully TRANSLATED into ENGLISH Metre. Whereunto is prefixed a discourse de\u2223claring not only the lawfullnes, but also the necessity of the heavenly Ordinance of singing Scripture Psalmes in the Churches of God. Coll.  III. \\n Let the word of God dwell plenteously in you, in all wisdome, teaching and exhort\u2223ing one another in Psalmes, Himnes, and spirituall Songs, singing to the Lord with grace in your hearts. Iames  v. \\n  any be afflicted, let him pray, and if any be merry let him sing psalmes. Imprinted 1640\n",
      "\n",
      "Page text preview: THE VVHOLE BOOKE OF PSALMES Faithfully TRANSLATED into ENGLISH Metre. Whereunto is prefixed a discourse de\u2223claring not only the lawfullnes, but also the necessity of the heavenly Ordinance of singing ...\n"
     ]
    }
   ],
   "source": [
    "# Load CSV Data\n",
    "\n",
    "# Find CSV files in data directory\n",
    "data_dir = Path('data')\n",
    "csv_files = list(data_dir.glob('*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Load and combine all CSV files\n",
    "all_data = []\n",
    "for csv_path in csv_files:\n",
    "    print(f\"\\nLoading {csv_path.name}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    all_data.append(df)\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "print(f\"\\nTotal combined rows: {len(combined_df)}\")\n",
    "print(f\"\\nFirst row sample:\")\n",
    "print(combined_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Model and Tokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "print(f\"Loading tokenizer from: {model_name}\")\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add custom [TIME] token\n",
    "if \"[TIME]\" not in tokenizer.vocab:\n",
    "    tokenizer.add_tokens([\"[TIME]\"])\n",
    "    print(\"Added [TIME] token to vocabulary\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Sample tokenization:\")\n",
    "\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"  Text: {sample_text}\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "print(f\"  Token IDs: {tokenizer.convert_tokens_to_ids(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5012 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5012/5012 [11:01<00:00,  7.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total pages extracted: 161599\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 161599 entries, 0 to 161598\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype\n",
      "---  ------     --------------   -----\n",
      " 0   author     161599 non-null  str  \n",
      " 1   place      161599 non-null  str  \n",
      " 2   date       161599 non-null  str  \n",
      " 3   page_text  161599 non-null  str  \n",
      "dtypes: str(4)\n",
      "memory usage: 4.9 MB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "    author                place             date  \\\n",
      "0  Unknown  [Cambridge, Mass. :  Imprinted 1640.   \n",
      "1  Unknown  [Cambridge, Mass. :  Imprinted 1640.   \n",
      "2  Unknown  [Cambridge, Mass. :  Imprinted 1640.   \n",
      "3  Unknown  [Cambridge, Mass. :  Imprinted 1640.   \n",
      "4  Unknown  [Cambridge, Mass. :  Imprinted 1640.   \n",
      "\n",
      "                                           page_text  \n",
      "0  THE VVHOLE BOOKE OF PSALMES Faithfully TRANSLA...  \n",
      "1  The Preface. THe singing of Psalmes, though it...  \n",
      "2              chron Reu. Reu. Num. Reu. Gal. chron.  \n",
      "3                                         Para s ers  \n",
      "4                                         Reu, deut.  \n",
      "\n",
      "\u2713 Saved to evan_pages_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Create Hugging Face Dataset and Apply Preprocessing\n",
    "\n",
    "def tokenize_and_chunk_function(examples, tokenizer, max_chunk_length=250):\n",
    "    \"\"\"\n",
    "    Efficiently tokenize and chunk text.\n",
    "    \n",
    "    Called with batched=True for speed.\n",
    "    Format: [CLS] <date> [TIME] <text_chunk> [SEP]\n",
    "    \"\"\"\n",
    "    batch_size = len(examples['date'])\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    \n",
    "    for idx in range(batch_size):\n",
    "        date = examples['date'][idx]\n",
    "        text = examples['page_text'][idx]\n",
    "        \n",
    "        if not text or pd.isna(text) or pd.isna(date):\n",
    "            continue\n",
    "        \n",
    "        date_str = str(date).strip()\n",
    "        text_str = str(text).strip()\n",
    "        \n",
    "        # Tokenize date and text\n",
    "        date_tokens = tokenizer.tokenize(date_str)\n",
    "        text_tokens = tokenizer.tokenize(text_str)\n",
    "        \n",
    "        # Create chunks of text with date prefix\n",
    "        for chunk_start in range(0, len(text_tokens), max_chunk_length):\n",
    "            chunk_end = min(chunk_start + max_chunk_length, len(text_tokens))\n",
    "            chunk_tokens = text_tokens[chunk_start:chunk_end]\n",
    "            \n",
    "            # Build sequence: [CLS] <date> [TIME] <text_chunk> [SEP]\n",
    "            tokens = [tokenizer.cls_token]\n",
    "            tokens.extend(date_tokens)\n",
    "            tokens.append(\"[TIME]\")\n",
    "            tokens.extend(chunk_tokens)\n",
    "            tokens.append(tokenizer.sep_token)\n",
    "            \n",
    "            # Convert to IDs\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            \n",
    "            if len(input_ids) <= 512:\n",
    "                all_input_ids.append(input_ids)\n",
    "                all_attention_masks.append(attention_mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': all_input_ids if all_input_ids else [],\n",
    "        'attention_mask': all_attention_masks if all_attention_masks else [],\n",
    "    }\n",
    "\n",
    "# Convert to HF Dataset and apply tokenization\n",
    "print(\"Converting to Hugging Face Dataset...\")\n",
    "dataset = Dataset.from_pandas(combined_df)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "print(\"\\nApplying tokenization and chunking with batched=True...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenize_and_chunk_function(examples, tokenizer, max_chunk_length=250),\n",
    "    batched=True,\n",
    "    batch_size=100,  # Process 100 examples at a time\n",
    "    remove_columns=['author', 'place', 'date', 'page_text'],\n",
    "    num_proc=2,  # Use 2 processes\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"\\nExample sample:\")\n",
    "example = tokenized_dataset[0]\n",
    "print(f\"  Input IDs length: {len(example['input_ids'])}\")\n",
    "print(f\"  Attention mask length: {len(example['attention_mask'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data and Set Up Training\n",
    "\n",
    "# Train/validation split (90/10)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Load the actual model\n",
    "print(f\"\\nLoading model: {model_name}\")\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Resize embeddings for [TIME] token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model resized for new vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Data collator with dynamic padding (key for efficiency!)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,  # Mask 15% of tokens\n",
    ")\n",
    "\n",
    "print(\"\\nData collator ready for dynamic padding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Training Arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output/bert_pretrained',\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=1,  # Start with 1 epoch for testing\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # Learning and optimization\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate once per epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Hardware\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Output directory: {training_args.output_dir}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Mixed precision: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871badea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized and ready for training!\")\n",
    "print(f\"Number of training steps per epoch: {len(train_dataset) // training_args.per_device_train_batch_size}\")\n",
    "print(f\"Total training steps: {len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeca9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"This may take a while depending on dataset size and hardware\")\n",
    "print()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4fd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "\n",
    "print(\"Evaluating model on validation data...\")\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_result.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load the Fine-tuned Model\n",
    "\n",
    "# Save the model\n",
    "output_dir = 'output/bert_pretrained'\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Model and tokenizer saved!\")\n",
    "print(f\"\\nSaved files:\")\n",
    "for file in Path(output_dir).glob('*'):\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "# Load the model back for inference\n",
    "print(\"\\nLoading model back for inference...\")\n",
    "loaded_model = BertForMaskedLM.from_pretrained(output_dir)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Inference with Masked Language Modeling\n",
    "\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Test sentence with masks\n",
    "test_sentence = \"[TIME] 1633 the quick brown fox jumps [MASK] the lazy dog\"\n",
    "print(f\"Test sentence: {test_sentence}\")\n",
    "\n",
    "# Tokenize\n",
    "encoded = loaded_tokenizer(test_sentence, return_tensors='pt')\n",
    "print(f\"\\nTokenized: {loaded_tokenizer.convert_ids_to_tokens(encoded['input_ids'][0].tolist())}\")\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**encoded)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Find the [MASK] token position\n",
    "mask_token_index = torch.where(encoded['input_ids'] == loaded_tokenizer.mask_token_id)[1]\n",
    "\n",
    "if len(mask_token_index) > 0:\n",
    "    mask_pos = mask_token_index[0].item()\n",
    "    predicted_token_id = predictions[0, mask_pos].argmax(axis=-1).item()\n",
    "    predicted_token = loaded_tokenizer.decode([predicted_token_id])\n",
    "    \n",
    "    print(f\"\\nMask position: {mask_pos}\")\n",
    "    print(f\"Predicted token: {predicted_token}\")\n",
    "    print(f\"Predicted token ID: {predicted_token_id}\")\n",
    "else:\n",
    "    print(\"No [MASK] token found in the sentence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}