{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ad6285",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Text-Machine/data-processing-code/blob/main/colab_training_gpt2_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d10cc",
   "metadata": {},
   "source": [
    "# GPT-2 Pretraining on Historical Texts (Google Colab)\n",
    "\n",
    "This notebook allows you to pretrain GPT-2 on historical text data (EEBO, ECCO, EVAN) using Google Colab's free GPU.\n",
    "\n",
    "**Key difference from chunk-based approach:** This uses sentence-based tokenization for more natural language boundaries.\n",
    "\n",
    "## Setup Steps:\n",
    "1. **Enable GPU**: Runtime → Change runtime type → GPU (T4 recommended)\n",
    "2. **Upload Data**: Upload your CSV files to Google Drive or upload directly\n",
    "3. **Run All Cells**: Runtime → Run all\n",
    "\n",
    "## What this does:\n",
    "- Installs required packages\n",
    "- Loads CSV data with columns: `author`, `place`, `date`, `page_text`\n",
    "- Splits text into sentences\n",
    "- Creates training sequences with `<date> [TIME]` prefix followed by one or more sentences\n",
    "- Trains GPT-2 with causal language modeling (next-token prediction)\n",
    "- Saves trained model to Google Drive\n",
    "\n",
    "## Why sentences instead of chunks?\n",
    "- **Natural boundaries**: Sentences respect linguistic structure\n",
    "- **Better context**: Models learn from semantically complete units\n",
    "- **Improved quality**: Fewer artificial breaks in the middle of clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. Go to Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73166f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets pandas accelerate nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2150158",
   "metadata": {},
   "source": [
    "## Option 1: Download Data from Google Drive\n",
    "\n",
    "If you have CSV files in Google Drive, uncomment and run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !gdown YOUR_FILE_ID_HERE\n",
    "# !unzip data.zip\n",
    "!gdown 11wfdV7j1TBv_i9XOiT8G8V4NxnJTxezz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import re\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK punkt tokenizer for sentence splitting\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(\"✓ NLTK punkt downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "\n",
    "def load_csv_as_dataset(csv_paths):\n",
    "    \"\"\"Load CSV files and convert to Hugging Face Dataset.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        logger.info(f\"Loading {Path(csv_path).name}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"  Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
    "        all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    logger.info(f\"Total rows: {len(combined_df)}\")\n",
    "    \n",
    "    dataset = Dataset.from_pandas(combined_df)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_sentence_function(examples, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize text and create training examples based on sentences.\n",
    "    \n",
    "    Format: <date> [TIME] <sentence1> <sentence2> ...\n",
    "    \n",
    "    Sentences are combined until we reach near max_length.\n",
    "    Each example has the date prefix for temporal context.\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "\n",
    "    time_id = tokenizer.convert_tokens_to_ids(\"[TIME]\")\n",
    "\n",
    "    for date, text in zip(examples[\"date\"], examples[\"page_text\"]):\n",
    "\n",
    "        if not text or pd.isna(text) or pd.isna(date):\n",
    "            continue\n",
    "\n",
    "        date_str = str(date).strip()\n",
    "        text_str = str(text).strip()\n",
    "\n",
    "        # Create prefix: \"<date> [TIME] \"\n",
    "        prefix = f\"{date_str} [TIME] \"\n",
    "        prefix_ids = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "        reserved_tokens = len(prefix_ids)  # Space reserved for prefix\n",
    "\n",
    "        # Split text into sentences\n",
    "        try:\n",
    "            sentences = sent_tokenize(text_str)\n",
    "        except:\n",
    "            # Fallback: split on periods if sent_tokenize fails\n",
    "            sentences = [s.strip() for s in text_str.split('.') if s.strip()]\n",
    "\n",
    "        if not sentences:\n",
    "            continue\n",
    "\n",
    "        # Group sentences to create training examples\n",
    "        current_ids = prefix_ids.copy()\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            \n",
    "            # Check if adding this sentence would exceed max_length\n",
    "            if len(current_ids) + len(sentence_ids) + 1 > max_length:  # +1 for space\n",
    "                # Save current example and start new one\n",
    "                if len(current_ids) > reserved_tokens:  # Only save if we have sentences\n",
    "                    input_ids_list.append(current_ids[:max_length])\n",
    "                    attention_masks_list.append([1] * len(input_ids_list[-1]))\n",
    "                \n",
    "                # Start new example with prefix and current sentence\n",
    "                current_ids = prefix_ids.copy()\n",
    "                current_ids.extend(sentence_ids)\n",
    "                current_ids.append(tokenizer.encode(\" \", add_special_tokens=False)[0])\n",
    "            else:\n",
    "                # Add sentence to current example\n",
    "                current_ids.extend(sentence_ids)\n",
    "                current_ids.append(tokenizer.encode(\" \", add_special_tokens=False)[0])\n",
    "        \n",
    "        # Save last example\n",
    "        if len(current_ids) > reserved_tokens:\n",
    "            input_ids_list.append(current_ids[:max_length])\n",
    "            attention_masks_list.append([1] * len(input_ids_list[-1]))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_masks_list,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d27795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 8      # Smaller due to GPT-2's memory requirements\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SAMPLES = None  # Set to e.g., 10000 for quick testing\n",
    "MAX_LENGTH = 512    # Max tokens per training example\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Max length: {MAX_LENGTH} tokens\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES or 'All'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad719bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = '.'\n",
    "csv_files = list(Path(DATA_DIR).glob('*.csv'))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}. Please upload data first.\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "dataset = load_csv_as_dataset(csv_files)\n",
    "print(f\"\\nDataset loaded: {len(dataset)} rows\")\n",
    "\n",
    "# Limit samples if specified\n",
    "if MAX_SAMPLES and MAX_SAMPLES < len(dataset):\n",
    "    dataset = dataset.select(range(MAX_SAMPLES))\n",
    "    print(f\"Limited to {MAX_SAMPLES} samples for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(\"Loading GPT-2 tokenizer and model...\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "# Add [TIME] special token\n",
    "if \"[TIME]\" not in tokenizer.vocab:\n",
    "    tokenizer.add_tokens([\"[TIME]\"])\n",
    "    print(\"Added [TIME] token to vocabulary\")\n",
    "\n",
    "# Set padding token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Set pad_token to: {tokenizer.pad_token}\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data (tokenize by sentences)\n",
    "print(\"Tokenizing and creating sentence-based training examples (this may take several minutes)...\")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenize_and_sentence_function(\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        max_length=MAX_LENGTH\n",
    "    ),\n",
    "    batched=True,\n",
    "    batch_size=50,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1,\n",
    "    desc=\"Tokenizing by sentences\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)} samples\")\n",
    "\n",
    "# Show sample\n",
    "if len(tokenized_dataset) > 0:\n",
    "    sample = tokenized_dataset[0]\n",
    "    print(f\"\\nSample input (first 100 tokens):\")\n",
    "    print(tokenizer.decode(sample['input_ids'][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e48044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "OUTPUT_DIR = 'gpt2_ecco_pretrained'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Data collator for causal language modeling (NOT masked LM)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # False = causal LM (GPT-2), True = masked LM (BERT)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer configured\")\n",
    "print(f\"\\nStarting training with {EPOCHS} epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00774580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e732f82",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "Let's test the model with text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512011b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test prompts with historical context\n",
    "test_prompts = [\n",
    "    \"1650 [TIME] The king\",\n",
    "    \"1800 [TIME] The parliament\",\n",
    "    \"1700 [TIME] In London\",\n",
    "]\n",
    "\n",
    "print(\"Testing text generation:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_length=60,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    for i, output in enumerate(outputs, 1):\n",
    "        generated_text = output['generated_text']\n",
    "        print(f\"  {i}. {generated_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6acfee",
   "metadata": {},
   "source": [
    "## Download Model (Optional)\n",
    "\n",
    "If you want to download the trained model to your local machine, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8053c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download model\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create zip file\n",
    "zip_path = '/content/gpt2_pretrained'\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model zipped. Size: {Path(f'{zip_path}.zip').stat().st_size / 1e6:.1f} MB\")\n",
    "print(\"Downloading...\")\n",
    "\n",
    "# Download\n",
    "files.download(f'{zip_path}.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
