{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Text-Machine/data-processing-code/blob/main/colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# BERT Pretraining on Historical Texts (Google Colab)\n",
    "\n",
    "This notebook allows you to pretrain BERT on historical text data (EEBO, ECCO, EVAN) using Google Colab's free GPU.\n",
    "\n",
    "## Setup Steps:\n",
    "1. **Enable GPU**: Runtime \u2192 Change runtime type \u2192 GPU (T4 recommended)\n",
    "2. **Upload Data**: Upload your CSV files to Google Drive or upload directly\n",
    "3. **Run All Cells**: Runtime \u2192 Run all\n",
    "\n",
    "## What this does:\n",
    "- Installs required packages\n",
    "- Mounts Google Drive (optional)\n",
    "- Loads CSV data with columns: `author`, `place`, `date`, `page_text`\n",
    "- Chunks text into 250-token segments with `[TIME] <date>` prefix\n",
    "- Trains BERT with masked language modeling\n",
    "- Saves trained model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Go to Runtime \u2192 Change runtime type \u2192 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets pandas accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional - for loading data and saving models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths (adjust these to your Google Drive structure)\n",
    "DATA_DIR = '/content/drive/MyDrive/text-machine-data'  # Where your CSV files are\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/bert-pretrained'  # Where to save the model\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_options"
   },
   "source": [
    "## Option 1: Use Data from Google Drive\n",
    "\n",
    "If you have CSV files in Google Drive, use the cell above.\n",
    "\n",
    "## Option 2: Upload Data Directly\n",
    "\n",
    "Run the cell below to upload CSV files directly to Colab (note: files will be deleted when runtime disconnects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_files"
   },
   "outputs": [],
   "source": [
    "# Option 2: Upload files directly to Colab\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(\"Upload your CSV files (must have columns: author, place, date, page_text)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to data directory\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'data/{filename}')\n",
    "    print(f\"Uploaded: data/{filename} ({len(uploaded[filename])/1e6:.1f} MB)\")\n",
    "\n",
    "# Update paths\n",
    "DATA_DIR = 'data'\n",
    "OUTPUT_DIR = 'output/bert_pretrained'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_functions"
   },
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "\n",
    "def load_csv_as_dataset(csv_paths):\n",
    "    \"\"\"Load CSV files and convert to Hugging Face Dataset.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        logger.info(f\"Loading {Path(csv_path).name}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"  Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
    "        all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    logger.info(f\"Total rows: {len(combined_df)}\")\n",
    "    \n",
    "    dataset = Dataset.from_pandas(combined_df)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_and_chunk_function(examples, tokenizer, max_chunk_length=250):\n",
    "    \"\"\"Tokenize text and create chunks with date prefix.\"\"\"\n",
    "    batch_size = len(examples['date'])\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    \n",
    "    for idx in range(batch_size):\n",
    "        date = examples['date'][idx]\n",
    "        text = examples['page_text'][idx]\n",
    "        \n",
    "        if not text or pd.isna(text) or pd.isna(date):\n",
    "            continue\n",
    "        \n",
    "        date_str = str(date).strip()\n",
    "        text_str = str(text).strip()\n",
    "        \n",
    "        # Tokenize date and text\n",
    "        date_tokens = tokenizer.tokenize(date_str)\n",
    "        text_tokens = tokenizer.tokenize(text_str)\n",
    "        \n",
    "        # Create chunks: [CLS] [TIME] <date> <text_chunk> [SEP]\n",
    "        for chunk_start in range(0, len(text_tokens), max_chunk_length):\n",
    "            chunk_end = min(chunk_start + max_chunk_length, len(text_tokens))\n",
    "            chunk_tokens = text_tokens[chunk_start:chunk_end]\n",
    "            \n",
    "            tokens = [tokenizer.cls_token, \"[TIME]\"] + date_tokens + chunk_tokens + [tokenizer.sep_token]\n",
    "            \n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            \n",
    "            if len(input_ids) <= 512:\n",
    "                all_input_ids.append(input_ids)\n",
    "                all_attention_masks.append(attention_mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_masks,\n",
    "    }\n",
    "\n",
    "print(\"\u2713 Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configuration"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CHUNK_LENGTH = 250\n",
    "BATCH_SIZE = 16  # Reduce if you run out of memory\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_SAMPLES = None  # Set to e.g., 10000 for quick testing\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Chunk length: {CHUNK_LENGTH} tokens\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES or 'All'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "csv_files = list(Path(DATA_DIR).glob('*.csv'))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}. Please upload data first.\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "dataset = load_csv_as_dataset(csv_files)\n",
    "print(f\"\\nDataset loaded: {len(dataset)} rows\")\n",
    "\n",
    "# Limit samples if specified\n",
    "if MAX_SAMPLES and MAX_SAMPLES < len(dataset):\n",
    "    dataset = dataset.select(range(MAX_SAMPLES))\n",
    "    print(f\"Limited to {MAX_SAMPLES} samples for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(\"Loading BERT tokenizer and model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Add [TIME] special token\n",
    "if \"[TIME]\" not in tokenizer.vocab:\n",
    "    tokenizer.add_tokens([\"[TIME]\"])\n",
    "    print(\"Added [TIME] token to vocabulary\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess"
   },
   "outputs": [],
   "source": [
    "# Preprocess data (tokenize and chunk)\n",
    "print(\"Tokenizing and chunking text (this may take several minutes)...\")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenize_and_chunk_function(\n",
    "        examples, \n",
    "        tokenizer, \n",
    "        max_chunk_length=CHUNK_LENGTH\n",
    "    ),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=['author', 'place', 'date', 'page_text'],\n",
    "    num_proc=2,\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)} samples\")\n",
    "\n",
    "# Show sample\n",
    "if len(tokenized_dataset) > 0:\n",
    "    sample = tokenized_dataset[0]\n",
    "    print(f\"\\nSample input (first 100 tokens):\")\n",
    "    print(tokenizer.decode(sample['input_ids'][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_data"
   },
   "outputs": [],
   "source": [
    "# Split train/validation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "outputs": [],
   "source": [
    "# Setup training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Trainer configured\")\n",
    "print(f\"\\nStarting training with {EPOCHS} epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"\\n\u2713 Model saved successfully!\")\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"  from transformers import BertForMaskedLM, BertTokenizer\")\n",
    "print(f\"  model = BertForMaskedLM.from_pretrained('{OUTPUT_DIR}')\")\n",
    "print(f\"  tokenizer = BertTokenizer.from_pretrained('{OUTPUT_DIR}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_section"
   },
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "Let's test the model with masked language modeling predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "# Test MLM predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test sentences with historical context\n",
    "test_sentences = [\n",
    "    \"[TIME] 1650 The king [MASK] to parliament.\",\n",
    "    \"[TIME] 1700 The book was [MASK] in London.\",\n",
    "    \"[TIME] 1600 He was a [MASK] man.\",\n",
    "]\n",
    "\n",
    "print(\"Testing masked language model predictions:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    print(f\"Input: {sentence}\")\n",
    "    predictions = fill_mask(sentence, top_k=3)\n",
    "    for i, pred in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {pred['token_str']:>12} (score: {pred['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## Download Model (Optional)\n",
    "\n",
    "If you want to download the trained model to your local machine, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# Zip and download model\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create zip file\n",
    "zip_path = '/content/bert_pretrained'\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model zipped. Size: {Path(f'{zip_path}.zip').stat().st_size / 1e6:.1f} MB\")\n",
    "print(\"Downloading...\")\n",
    "\n",
    "# Download\n",
    "files.download(f'{zip_path}.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}